# ======================================================================
# == reformat_dataset.py (–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –£–ü–†–û–©–ï–ù–ò–ï–ú –ø—Ä–æ–º–ø—Ç–∞)
# ======================================================================
import json
import os
from pathlib import Path

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SOURCE_FILE = Path("distillation_dataset.jsonl")
OUTPUT_DIR = Path("./lora_data")
TRAIN_RATIO = 0.95 # 95% –¥–∞–Ω–Ω—ã—Ö –ø–æ–π–¥—É—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, 5% –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É
# -----------------

def reformat_and_simplify():
    print("="*50 + "\n--- –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö ---\n" + "="*50)

    if not SOURCE_FILE.exists():
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª {SOURCE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    OUTPUT_DIR.mkdir(exist_ok=True)
    train_path = OUTPUT_DIR / "train.jsonl"
    valid_path = OUTPUT_DIR / "valid.jsonl"

    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.")
    
    reformatted_data = []
    for item in data:
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –°–£–¢–¨ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            user_content_json = json.loads(item["messages"][0]["content"].replace("–ê–Ω–∞–ª–∏–∑ —Å–∏–≥–Ω–∞–ª–∞: ", ""))
            assistant_content = item["messages"][1]["content"]

            # --- [–ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï] ---
            # –°–æ–∑–¥–∞–µ–º –ö–û–†–û–¢–ö–ò–ô, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            symbol = user_content_json.get("symbol", "N/A")
            side = user_content_json.get("side", "N/A")
            source = user_content_json.get("logic", "N/A") # –í —Å—Ç–∞—Ä–æ–º —Ñ–∞–π–ª–µ —ç—Ç–æ 'logic'
            user_prompt = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–∏–≥–Ω–∞–ª {side.upper()} –¥–ª—è {symbol} –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}."
            # ---------------------------

            # –°–æ–±–∏—Ä–∞–µ–º –æ–¥–Ω—É –±–æ–ª—å—à—É—é —Å—Ç—Ä–æ–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ Llama 3
            formatted_text = (
                f"<|start_header_id|>user<|end_header_id|>\n\n"
                f"{user_prompt}<|eot_id|>"
                f"<|start_header_id|>assistant<|end_header_id|>\n\n"
                f"{assistant_content}<|eot_id|>"
            )
            reformatted_data.append({"text": formatted_text})
        except Exception:
            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—à–∏–±–∫–∞–º–∏

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    split_index = int(len(reformatted_data) * 0.95)
    train_data = reformatted_data[:split_index]
    valid_data = reformatted_data[split_index:]

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª—ã
    with open(train_path, "w", encoding="utf-8") as f:
        for item in train_data: f.write(json.dumps(item) + "\n")
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {train_path.resolve()} ({len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")

    with open(valid_path, "w", encoding="utf-8") as f:
        for item in valid_data: f.write(json.dumps(item) + "\n")
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {valid_path.resolve()} ({len(valid_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
    
    print("\n" + "="*50)
    print("üéâ –î–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤—ã –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è!")
    print("="*50)

if __name__ == "__main__":
    reformat_and_simplify()