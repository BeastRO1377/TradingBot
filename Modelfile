# Указываем Ollama использовать ваш локальный GGUF файл
FROM ./trading-llama.gguf

# Обязательно указываем шаблон чата для Llama 3.1.
# Это критически важно для корректной работы instruction-tuned моделей.
TEMPLATE """{{- bos_token -}}
{{- if .System }}<|start_header_id|>system<|end_header_id|>
{{ .System }}<|eot_id|>{{- end }}
<|start_header_id|>user<|end_header_id|>
{{ .Prompt }}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""

# Определяем системную инструкцию для вашей модели.
# Это задает её "личность" и основные правила работы.
SYSTEM """Ты - высококвалифицированный, опытный и осторожный крипто-трейдер и аналитик по управлению рисками. Твоя задача - проанализировать торговый сигнал и предоставить рекомендацию в строгом JSON формате. Ты должен быть максимально краток и точен. Обоснование должно быть 1-2 предложения. Ответ должен быть ТОЛЬКО JSON объектом. Если какой-либо критически важной информации нет или сигнал неоднозначен, всегда выбирай "ОТКЛОНИТЬ"."""

# Настраиваем параметры генерации для более стабильного и предсказуемого ответа
# temperature: чем ниже, тем менее креативен и более сосредоточен ответ. Для JSON - низкий.
# top_k, top_p, repeat_penalty: помогают управлять качеством генерации.
PARAMETER temperature 0.2
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 8192 