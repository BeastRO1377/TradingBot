# ==============================================================================
# == finetune.py (–ï–î–ò–ù–´–ô, –°–ê–ú–û–î–û–°–¢–ê–¢–û–ß–ù–´–ô –°–ö–†–ò–ü–¢ –î–õ–Ø –î–û–û–ë–£–ß–ï–ù–ò–Ø)
# ==============================================================================
import argparse
import json
import logging
import time
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
from mlx_lm.utils import load, save_config
from tqdm import tqdm

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—É—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º
MODEL_PATH = "./Meta-Llama-3.1-8B-Instruct"
DATA_PATH = "./training_data_final.jsonl" # –§–∞–π–ª, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Å–æ–∑–¥–∞–ª–∏ —Å –ø–æ–º–æ—â—å—é reformat_dataset.py
ADAPTER_PATH = "./mlx_adapters"
NUM_EPOCHS = 1 # –î–ª—è ~500 –ø—Ä–∏–º–µ—Ä–æ–≤ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ –±–æ–ª–µ–µ —á–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
LEARNING_RATE = 1e-5
# -----------------

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

class Dataset:
    def __init__(self, path: Path, tokenizer):
        if not path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {path}")
        
        with open(path, "r") as fid:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É
            self.data = [json.loads(line) for line in fid]
        
        self.tokenizer = tokenizer

    def __getitem__(self, idx):
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ª–µ—Ç—É
        text = self.data[idx]["text"]
        return self.tokenizer(
            text,
            return_tensors="np",
            return_attention_mask=False,
        )["input_ids"]

    def __len__(self):
        return len(self.data)

def main(args):
    logger.info("="*50)
    logger.info("--- –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è ---")
    logger.info("="*50)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏–∑: {args.model}")
    model, tokenizer = load(args.model)

    # 2. "–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º" –≤—Å–µ —Å–ª–æ–∏ –º–æ–¥–µ–ª–∏
    model.freeze()
    # "–†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º" –∏ –∑–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Å–ª–æ–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –¥–æ–æ–±—É—á–∞—Ç—å (LoRA)
    for _, m in model.named_modules():
        if isinstance(m, nn.Linear):
            m.set_lora()
    
    logger.info("–ú–æ–¥–µ–ª—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –¥–ª—è LoRA –¥–æ–æ–±—É—á–µ–Ω–∏—è.")

    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {args.data}")
    train_dataset = Dataset(Path(args.data), tokenizer)

    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    loss_and_grad_fn = nn.value_and_grad(model, lambda x, y: nn.losses.cross_entropy(x, y).mean())
    optimizer = optim.Adam(learning_rate=args.learning_rate)

    # 5. –¶–∏–∫–ª –¥–æ–æ–±—É—á–µ–Ω–∏—è
    for epoch in range(args.num_epochs):
        start_time = time.time()
        total_loss = 0
        
        pbar = tqdm(range(len(train_dataset)), desc=f"–≠–ø–æ—Ö–∞ {epoch + 1}/{args.num_epochs}")
        for i in pbar:
            sample = train_dataset[i]
            x = mx.array(sample[:, :-1])
            y = mx.array(sample[:, 1:])

            loss, grads = loss_and_grad_fn(model, x, y)
            optimizer.update(model, grads)
            mx.eval(model.parameters(), optimizer.state)
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{total_loss / (i + 1):.5f}"})
        
        epoch_time = time.time() - start_time
        logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {epoch_time:.2f}—Å. –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {total_loss / len(train_dataset):.5f}")

    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    adapter_path = Path(args.adapter_path)
    adapter_path.mkdir(parents=True, exist_ok=True)
    model.save_weights(str(adapter_path / "adapters.safetensors"))
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ–±—ã `fuse.py` –µ–≥–æ –Ω–∞—à–µ–ª
    tokenizer.save_pretrained(adapter_path)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥ LoRA
    with open(adapter_path / "adapter_config.json", "w") as f:
        json.dump({"lora_layers": -1}, f, indent=4)

    logger.info("="*50)
    logger.info("‚úÖüéâ –î–æ–æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"–ê–¥–∞–ø—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {adapter_path.resolve()}")
    logger.info("="*50)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–°–∫—Ä–∏–ø—Ç –¥–ª—è LoRA –¥–æ–æ–±—É—á–µ–Ω–∏—è.")
    # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —á—Ç–æ–±—ã –Ω–µ –ø–∏—Å–∞—Ç—å –∏—Ö –≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
    parser.add_argument("--model", type=str, default=MODEL_PATH, help="–ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
    parser.add_argument("--data", type=str, default=DATA_PATH, help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--adapter-path", type=str, default=ADAPTER_PATH, help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤")
    parser.add_argument("--num-epochs", type=int, default=NUM_EPOCHS, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--learning-rate", type=float, default=LEARNING_RATE, help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è")
    
    args = parser.parse_args()
    main(args)