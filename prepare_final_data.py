# ======================================================================
# == prepare_final_data.py (–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ñ–ï–°–¢–ö–ò–ú –õ–ò–ú–ò–¢–û–ú)
# ======================================================================
import json
from pathlib import Path
import transformers
import logging

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SOURCE_FILE = Path("distillation_dataset.jsonl")
OUTPUT_DIR = Path("./lora_data_final")
TOKENIZER_PATH = "./Meta-Llama-3.1-8B-Instruct" # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç Llama, –æ–Ω —Å–æ–≤–º–µ—Å—Ç–∏–º
# [–ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï] –ñ–µ—Å—Ç–∫–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏
MAX_LENGTH = 1024 
# -----------------

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

def prepare_and_truncate_data():
    logger.info("="*50)
    logger.info("--- –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏ –û–ë–†–ï–ó–ö–ò –¥–∞–Ω–Ω—ã—Ö ---")
    logger.info("="*50)

    # ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Å–∫—Ä–∏–ø—Ç–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    # (–û–Ω –±—É–¥–µ—Ç –∑–¥–µ—Å—å –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã)
    if not SOURCE_FILE.exists():
        logger.error(f"‚ùå –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª {SOURCE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    if not Path(TOKENIZER_PATH).exists():
        logger.error(f"‚ùå –ü–∞–ø–∫–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º {TOKENIZER_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return

    OUTPUT_DIR.mkdir(exist_ok=True)
    train_path = OUTPUT_DIR / "train.jsonl"
    valid_path = OUTPUT_DIR / "valid.jsonl"

    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª–∏–Ω—ã...")
    tokenizer = transformers.AutoTokenizer.from_pretrained(TOKENIZER_PATH)

    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]
    logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.")
    
    processed_data = []
    truncated_count = 0
    for i, item in enumerate(data):
        try:
            user_content = item["messages"][0]["content"]
            assistant_content = item["messages"][1]["content"]

            full_text = (
                f"<|start_header_id|>user<|end_header_id|>\n\n{user_content}<|eot_id|>"
                f"<|start_header_id|>assistant<|end_header_id|>\n\n{assistant_content}<|eot_id|>"
            )

            token_ids = tokenizer.encode(full_text)
            if len(token_ids) > MAX_LENGTH:
                token_ids = token_ids[:MAX_LENGTH]
                truncated_count += 1
            
            final_text = tokenizer.decode(token_ids)
            processed_data.append({"text": final_text})

        except Exception as e:
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏ {i+1} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏: {e}")
            continue

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–µ–∑–∞–Ω–æ {truncated_count} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.")

    split_index = int(len(processed_data) * 0.95)
    train_data = processed_data[:split_index]
    valid_data = processed_data[split_index:]

    with open(train_path, "w", encoding="utf-8") as f:
        for item in train_data: f.write(json.dumps(item) + "\n")
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {train_path.resolve()} ({len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")

    with open(valid_path, "w", encoding="utf-8") as f:
        for item in valid_data: f.write(json.dumps(item) + "\n")
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {valid_path.resolve()} ({len(valid_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
    
    print("\n" + "="*50)
    print("üéâ –î–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤—ã –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è!")
    print("="*50)

if __name__ == "__main__":
    prepare_and_truncate_data()