#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –∫–æ–¥–µ TradingBot
"""

import os
import hashlib
import shutil
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime

def get_file_hash(filepath):
    """–í—ã—á–∏—Å–ª—è–µ—Ç MD5 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    hash_md5 = hashlib.md5()
    try:
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        return None

def backup_file(filepath, backup_dir):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Ñ–∞–π–ª–∞"""
    try:
        backup_path = backup_dir / filepath.name
        shutil.copy2(filepath, backup_path)
        return backup_path
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ {filepath}: {e}")
        return None

def find_exact_duplicates():
    """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É"""
    current_dir = Path.cwd()
    hash_groups = defaultdict(list)
    
    # –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã MultiuserBot*
    for root, dirs, files in os.walk(current_dir):
        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'node_modules']]
        
        for file in files:
            if file.startswith('MultiuserBot') and file.endswith('.py'):
                filepath = Path(root) / file
                file_hash = get_file_hash(filepath)
                if file_hash:
                    hash_groups[file_hash].append(filepath)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
    duplicates = {hash_val: files for hash_val, files in hash_groups.items() if len(files) > 1}
    return duplicates

def cleanup_duplicates(dry_run=True, create_backup=True):
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç–∫–∞—Ç–∞"""
    print("üßπ –û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –∫–æ–¥–µ TradingBot")
    print("=" * 60)
    
    if dry_run:
        print("üîç –†–ï–ñ–ò–ú –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–û–ì–û –ü–†–û–°–ú–û–¢–†–ê (—Ñ–∞–π–ª—ã –Ω–µ –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã)")
    else:
        print("‚ö†Ô∏è  –†–ï–ñ–ò–ú –†–ï–ê–õ–¨–ù–û–ì–û –£–î–ê–õ–ï–ù–ò–Ø")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
    backup_dir = None
    if create_backup and not dry_run:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = Path(f"backup_duplicates_{timestamp}")
        backup_dir.mkdir(exist_ok=True)
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {backup_dir}")
    
    # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = find_exact_duplicates()
    
    if not duplicates:
        print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return
    
    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
    
    total_files_to_remove = 0
    total_size_to_save = 0
    cleanup_plan = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    for file_hash, files in duplicates.items():
        print(f"\nüîÑ –ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Ö–µ—à: {file_hash[:8]}...):")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –¥–∞—Ç–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π)
        files_with_time = []
        for filepath in files:
            try:
                stat = os.stat(filepath)
                files_with_time.append((filepath, stat.st_mtime, stat.st_size))
            except:
                continue
        
        files_with_time.sort(key=lambda x: x[1], reverse=True)  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        
        # –ü–µ—Ä–≤—ã–π —Ñ–∞–π–ª (—Å–∞–º—ã–π –Ω–æ–≤—ã–π) –æ—Å—Ç–∞–≤–ª—è–µ–º, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–¥–∞–ª—è–µ–º
        keep_file = files_with_time[0]
        remove_files = files_with_time[1:]
        
        print(f"   ‚úÖ –û—Å—Ç–∞–≤–ª—è–µ–º: {keep_file[0].name} (—Ä–∞–∑–º–µ—Ä: {keep_file[2]} –±–∞–π—Ç)")
        
        for filepath, mtime, size in remove_files:
            print(f"   üóëÔ∏è  –£–¥–∞–ª—è–µ–º: {filepath.name} (—Ä–∞–∑–º–µ—Ä: {size} –±–∞–π—Ç)")
            total_files_to_remove += 1
            total_size_to_save += size
            
            if not dry_run:
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
                if create_backup:
                    backup_path = backup_file(filepath, backup_dir)
                    if backup_path:
                        print(f"      üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}")
                
                # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
                try:
                    os.remove(filepath)
                    print(f"      ‚úÖ –§–∞–π–ª —É–¥–∞–ª–µ–Ω")
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
        
        cleanup_plan.append({
            'hash': file_hash,
            'keep': str(keep_file[0]),
            'remove': [str(f[0]) for f in remove_files],
            'size_saved': sum(f[2] for f in remove_files)
        })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω –æ—á–∏—Å—Ç–∫–∏
    if not dry_run:
        cleanup_log = {
            'timestamp': datetime.now().isoformat(),
            'total_files_removed': total_files_to_remove,
            'total_size_saved_bytes': total_size_to_save,
            'total_size_saved_kb': total_size_to_save / 1024,
            'backup_directory': str(backup_dir) if backup_dir else None,
            'cleanup_plan': cleanup_plan
        }
        
        cleanup_log_path = Path(f"cleanup_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(cleanup_log_path, 'w', encoding='utf-8') as f:
            json.dump(cleanup_log, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìù –õ–æ–≥ –æ—á–∏—Å—Ç–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {cleanup_log_path}")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –§–∞–π–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {total_files_to_remove}")
    print(f"   –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {total_size_to_save / 1024:.1f} KB")
    
    if backup_dir:
        print(f"   –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏: {backup_dir}")
        print(f"   üí° –î–ª—è –æ—Ç–∫–∞—Ç–∞: —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –∏–∑ {backup_dir} –æ–±—Ä–∞—Ç–Ω–æ")
    
    if dry_run:
        print(f"\nüîç –≠—Ç–æ –±—ã–ª –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä.")
        print(f"   –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python3 {__file__} --execute")
    else:
        print(f"\n‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files_to_remove}")
        print(f"   üíæ –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {backup_dir}")

def main():
    import sys
    
    dry_run = True
    create_backup = True
    
    if len(sys.argv) > 1:
        if sys.argv[1] == '--execute':
            dry_run = False
            print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Ä–µ–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤!")
            response = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (y/N): ")
            if response.lower() != 'y':
                print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
                return
        elif sys.argv[1] == '--no-backup':
            create_backup = False
        elif sys.argv[1] == '--help':
            print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
            print("  python3 cleanup_duplicates.py          # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä")
            print("  python3 cleanup_duplicates.py --execute # –†–µ–∞–ª—å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ")
            print("  python3 cleanup_duplicates.py --no-backup # –ë–µ–∑ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π")
            return
    
    cleanup_duplicates(dry_run=dry_run, create_backup=create_backup)

if __name__ == "__main__":
    main()





























