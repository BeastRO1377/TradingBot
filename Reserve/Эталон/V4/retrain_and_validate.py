# retrain_and_validate.py
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import joblib
import shutil
import time  # <--- –î–û–ë–ê–í–õ–ï–ù –ò–°–ü–†–ê–í–õ–Ø–Æ–©–ò–ô –ò–ú–ü–û–†–¢
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º numpy.NaN, –µ—Å–ª–∏ –∞—Ç—Ä–∏–±—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
if not hasattr(np, "NaN"):
    np.NaN = np.nan

import config
from ai_ml import train_golden_model_mlx, save_mlx_checkpoint, MLXInferencer

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def prepare_training_data(csv_path: Path) -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –≥–æ—Ç–æ–≤–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ trades_unified.csv."""
    if not csv_path.exists():
        logging.error(f"–§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        return []

    df = pd.read_csv(csv_path)
    df_filtered = df[df['event'] == 'open'].copy()
    
    close_events = df[df['event'] == 'close']
    targets = []
    for _, open_row in df_filtered.iterrows():
        corresponding_close = close_events[
            (close_events['symbol'] == open_row['symbol']) &
            (close_events['timestamp'] > open_row['timestamp'])
        ].sort_values('timestamp').iloc[:1]
        
        if not corresponding_close.empty:
            pnl_pct = corresponding_close['pnl_pct'].values[0]
            targets.append(1 if pnl_pct > 0.5 else 0)
        else:
            targets.append(np.nan)

    df_filtered['target'] = targets
    df_filtered.dropna(subset=['target'], inplace=True)
    
    training_data = []
    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–ª—é—á–∏ —Ñ–∏—á–µ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ ---
    feature_keys = [
        "price", "open_interest", "volume_1m", "rsi14", "adx14", "volume_anomaly"
    ]
    for _, row in df_filtered.iterrows():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å KeyError
        if not all(key in row for key in feature_keys):
            continue
            
        features = [row[k] for k in feature_keys]
        if not all(isinstance(f, (int, float)) and np.isfinite(f) for f in features):
            continue
        training_data.append({"features": features, "target": row['target']})
        
    return training_data

def evaluate_model(inferencer: MLXInferencer, X_test: np.ndarray, y_test: np.ndarray) -> dict:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    if inferencer.model is None or X_test.size == 0:
        return {"accuracy": 0, "f1": 0, "precision": 0, "recall": 0}

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    raw_predictions = inferencer.infer(X_test)
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–≥–º–æ–∏–¥—É –∏ –ø–æ—Ä–æ–≥ 0.5 –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    probabilities = 1 / (1 + np.exp(-raw_predictions))
    predictions = (probabilities > 0.5).astype(int).flatten()

    # –î–æ–±–∞–≤–∏–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –Ω–µ—Ç –æ–¥–Ω–æ–≥–æ –∏–∑ –∫–ª–∞—Å—Å–æ–≤ (–¥–ª—è precision/recall)
    try:
        precision = precision_score(y_test, predictions, zero_division=0)
        recall = recall_score(y_test, predictions, zero_division=0)
        f1 = f1_score(y_test, predictions, zero_division=0)
    except Exception:
        precision, recall, f1 = 0, 0, 0

    return {
        "accuracy": accuracy_score(y_test, predictions),
        "f1": f1,
        "precision": precision,
        "recall": recall,
    }

if __name__ == "__main__":
    logging.info("--- [–ù–ê–ß–ê–õ–û] –ü—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ ---")

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    all_data = prepare_training_data(config.TRADES_UNIFIED_CSV_PATH)
    if len(all_data) < 100:
        logging.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ({len(all_data)} —Å—ç–º–ø–ª–æ–≤). –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω.")
        exit()

    features = np.array([d["features"] for d in all_data])
    targets = np.array([d["target"] for d in all_data])

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (80/20)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º stratify –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö
    try:
        X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42, stratify=targets)
    except ValueError: # –ï—Å–ª–∏ –∫–∞–∫–æ–π-—Ç–æ –∫–ª–∞—Å—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω 1 —Ä–∞–∑, stratify –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç
        X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)

    train_data_split = [{"features": f, "target": t} for f, t in zip(X_train, y_train)]

    # 2. –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ("–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–∞")
    logging.info(f"–û–±—É—á–µ–Ω–∏–µ '–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–∞' –Ω–∞ {len(train_data_split)} —Å—ç–º–ø–ª–∞—Ö...")
    challenger_model, challenger_scaler = train_golden_model_mlx(train_data_split, num_epochs=50)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º "–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–∞" –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    challenger_model_path = config.ML_MODEL_PATH.with_suffix(".challenger.safetensors")
    challenger_scaler_path = config.SCALER_PATH.with_suffix(".challenger.pkl")
    save_mlx_checkpoint(challenger_model, challenger_scaler, str(challenger_model_path), str(challenger_scaler_path))

    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    challenger_inferencer = MLXInferencer(challenger_model_path, challenger_scaler_path)
    challenger_metrics = evaluate_model(challenger_inferencer, X_test, y_test)
    logging.info(f"–ú–µ—Ç—Ä–∏–∫–∏ '–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–∞' –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {challenger_metrics}")

    champion_metrics = {"accuracy": 0, "f1": 0}
    if config.ML_MODEL_PATH.exists() and config.SCALER_PATH.exists():
        champion_inferencer = MLXInferencer(config.ML_MODEL_PATH, config.SCALER_PATH)
        champion_metrics = evaluate_model(champion_inferencer, X_test, y_test)
        logging.info(f"–ú–µ—Ç—Ä–∏–∫–∏ '–ß–µ–º–ø–∏–æ–Ω–∞' –Ω–∞ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö: {champion_metrics}")
    else:
        logging.warning("–ú–æ–¥–µ–ª—å '–ß–µ–º–ø–∏–æ–Ω' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. '–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç' –±—É–¥–µ—Ç –ø–æ–≤—ã—à–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")

    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π - F1-score, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —É—á–∏—Ç—ã–≤–∞–µ—Ç –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã
    if challenger_metrics["f1"] > champion_metrics["f1"]:
        logging.warning("üèÜ '–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç' –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! –ü–æ–≤—ã—à–∞–µ–º –¥–æ '–ß–µ–º–ø–∏–æ–Ω–∞'.")
        
        # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å
        if config.ML_MODEL_PATH.exists():
            archive_path = config.ML_MODEL_PATH.with_suffix(f".archive-{int(time.time())}.safetensors")
            shutil.copy(config.ML_MODEL_PATH, archive_path)
            shutil.copy(config.SCALER_PATH, config.SCALER_PATH.with_suffix(f".archive-{int(time.time())}.pkl"))
            logging.info(f"–°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –∑–∞–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ –≤ {archive_path.name}")

        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å –Ω–æ–≤–æ–π
        shutil.move(challenger_model_path, config.ML_MODEL_PATH)
        shutil.move(challenger_scaler_path, config.SCALER_PATH)
        logging.info("–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞–∫ —Ä–∞–±–æ—á–∞—è.")
    else:
        logging.info("‚öñÔ∏è '–ß–µ–º–ø–∏–æ–Ω' –æ—Å—Ç–∞–µ—Ç—Å—è –ª—É—á—à–∏–º. '–ü—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç' –æ—Ç–∫–ª–æ–Ω–µ–Ω.")
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–µ—Ç–µ–Ω–¥–µ–Ω—Ç–∞
        challenger_model_path.unlink()
        challenger_scaler_path.unlink()

    logging.info("--- [–ö–û–ù–ï–¶] –ü—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω ---")